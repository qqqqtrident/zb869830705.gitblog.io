---
layout: post
author: Robin
title: Python机器学习Ch2-05 --- 使用Python实现自适应线性神经元
tags: 机器学习, Python
categories:
  - 机器学习 
  - Python
---

从前一节已经了解到，感知机和Adaline的学习规则非常的相似，因此在实现Adaline的时候，我们可以重用感知机实现，对感知机实现进行修改即可得到Adaline，具体我们需要修改**fit**方法，并实现梯度下降算法，实现代码如下：

![](/assets/adaline-python-code.png)

从代码中可以看到，Adaline不像感知机那样每次用一个训练样本来更新权重参数，而是基于整个训练集的梯度来更新权重。

> 注意：**X.T.dot(errors)是一个矩阵和向量的乘法计算：**
> 
> $$\begin{bmatrix}{1}&{2}&{3}\\{4}&{5}&{6}\\\end{bmatrix} \times\begin{bmatrix}{7}\\{8}\\{9}\\
\end{bmatrix} \\
> = \begin{bmatrix}{1\times7}+&{2\times8}+&{3\times9}\\{4\times7}+&{5\times8}+&{6\times9}\\\end{bmatrix} \\
> = \begin{bmatrix}{50}\\{122}\\
\end{bmatrix}$$


在将Adaline应用到实际问题之前，需要确定一个好的学习率\\(\eta\\)，以保证算法真正的收敛。在这里先做一个实验，假设有两个不同的\\(\eta\\)值：\\(\eta = 0.01, \eta = 0.0001\\)，然后将每一轮的损失函数值绘制出来，查看一下Adaline是如何学习的（数据集依然使用感知机部分的Iris数据集以及加载方式）。

> 学习率**\\(\eta\\)**，迭代数**n_iter**也被称为**超参数（hyperparameters）**，超参数对于模型非常重要，在后面我们讲学习一些技巧，如何自动找到能够使模型效果最好的超参数。

![](/assets/learning-rate-test.png)

分析上面的最终图示可以看到，左边的图根本不是在最小化损失函数，而是在每一次迭代过程中，损失函数的值不断地增大，这说明取值过大的学习率不但对算法毫无益处，反而会造成算法计算结果严重失误；而右边的图能够在每一次迭代过程中不断的减小损失函数的值，但是问题在于，减小的幅度太小，导致收敛可能需要非常多次的迭代才能完成，因此过小的学习率反而造成算法收敛的耗时过长，使得算法不能解决实际的问题。


下图左侧展示了权重更新过程中如何得到损失函数\\(J(w)\\)的最小值的。右图展示了学习率过大时权重更新情况，每次都跳过了最小损失函数对应的权重值：

![](/assets/cost-func.jpg)

许多机器学习算法都要先对特征进行某种缩放操作，比如**标准化（standardization）**和**归一化（normalization）**。而缩放后的特征通常更有助于算法收敛，实际上，对特征进行缩放后再运用梯度下降算法往往会有更好的学习效果。

特征的标准化计算相对比较简单，比如要对第*j*维度的特征进行标准化，只需要计算所有训练集样本中第*j*维度的平均值\\(\mu_j\\)和标准差\\(\sigma_j\\)即可，然后套用公式：

$$x_j^{'} = \frac {x_j - \mu_j}{\sigma_j}$$

最终得到特征的均值为**0**，标准差为**1**。

在NumPy中，可以直接调用方法mean和std对数据特征进行标准化：

![](/assets/standardization.png)

数据标准化之后，我们使用Adaline算法来训练自适应线性神经元模型，看看收敛情况如何（学习率为0.01）：

![](/assets/adaline-training.png)


有最终的输出结果可知，标准化后的数据使用梯度下降Adaline算法，学习率为0.01的情况下，最终是收敛的，并且由图中可以看到**Sum-squared-error(SSE：\\(y - w^Tx\\))**最后并没有等于0，说明所有的样本都正确的被分类了。


> 文章内容来自《Python Machine Learning》
> 
> 由于正在学习，因此在记录过程中难免有误，请不吝指正批评，谢谢！