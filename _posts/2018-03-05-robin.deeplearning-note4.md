---
layout: post
author: Robin
title: 【Robin.DeepLearning】 四、神经网络之网络简单剖析
tags: 机器学习
categories:
  - 机器学习
---

## 3.1 神经网络剖析

训练一个神经网络模型，始终会有下面几个对象：

* Layer：layer可以组成一个网络（network）或者模型（model）；
* 输入数据和相应的目标；
* 损失函数：损失函数定义深度学习中的反馈信号；
* 优化器：优化器决定深度学习的行进方向。

如下图，可视化上述几个对象间的交互：网络是有多个layer连接组成的，将输入数据映射到输出的预测数据上。损失函数则比较预测值和目标之间的差值，即损失值。优化器使用损失值来调整网络权重，以最小化损失函数。

![](/assets/robin.deeplearning/4/note4_net.png)

### 3.1.1 Layer：深度学习的基础组件

神经网络的基础数据结构是Layer，Layer是将一个或者多个输入张量转换成一个或者多个输出张量的数据处理模块。有些layer是无状态的，但是更多的layer是带有状态的，layer的权重是有随机梯度下降学习的一个或者多个张量，他们包含了网络的基础知识（knowledge）。

不同的张量格式和数据类型需要不同的layer进行数据处理，比如，简单的向量数据存储成形状为（样本，特征）的2D张量，它经常用致密的全联接层（也称为全联接层或者致密层，Keras中的Dense类）处理。序列数据存储成形状为（样本，时间戳，特征）的3D张量，使用循环神经网络层（recurrent layer，比如，LSTM layer）处理。图片数据存储成4D张量，常用2D卷积层（Conv2D）处理。

Layer犹如深度学习的“乐高积木”，Keras通过剪辑拼接兼容的layer来构建深度学习模型，从而形成数据转换的管道。这里layer的兼容性特指：每类layer只接受一定形状的输入张量，返回一定形状的输出张量。例如下面的例子：

```python
from keras import layers
layer = layers.Dense(32, input_shape=(784,))
```

上面的例子中，创建了一个值接收2D张量的layer，该2D张量的第一个维度是784，返回的第一个维度是32的张量。

因此，这个layer联结的下游layer是以32维的向量作为输入的。使用Keras时无需担心layer的兼容性，因为加入到模型的layer会自动适配上一个layer的形状。例如：

```python
from keras import models
from keras import layers
model = models.Sequential()
model.add(layers.Dense(32, input_shape=(784,)))
model.add(layers.Dense(32))
```
其中第二个layer没有输入形状的参数，但是它可以自动推导其输入张量的形状是前一个layer的输出张量的形状。

### 3.1.2 模型：层的网络（networks of layers）

深度学习模型是一个有向无环图。最常见的例子是layer的逐层排列、线性连接，将单个的输入映射到单个的输出上。

经常还会有网络的拓扑结构，例如：

* Two-branch网络
* Multihead网络
* 感知机

网络组成的拓扑定义了假设空间（hypothesis space）。你可能还记得第一章中机器学习的定义：在预定的可能空间中，基于反馈信号寻找输入数据集的有效的表示。选定了一个网络拓扑，意味着限制了一系列张量操作的可行性空间（假设空间）。为权重张量搜索一组合适的值，将输入数据映射到输出数据。

### 3.1.3 损失函数和优化器：掌控学习的进度

一旦网络定义好，接着最要关注的是以下两件事：
* 损失函数（观察函数）：模型训练中损失函数值最小化，意味着任务的成功；
* 优化器：决定基于损失函数的神经网络如何迭代。它一般是随机梯度下降（stochastic gradient descent，SGD）的某种变体。

有多路输出的神经网络可能有多个损失函数（每路输出一个损失函数）。而梯度下降的过程必须基于单个标量损失值；所以，对于多损失函数的网络，所有损失函数被组合（一般通过平均）成单个标量值。

选择好正确的观察函数对于解决问题来说是至关重要的：你的网络尽可能找到最小化损失的捷径。所以，如果观察函数不能很好的与任务的成功相关联，那网络迭代结束并不会得到预想的结果。记住所有的神经网络都是最小化损失函数。

幸运地是，对于常见问题（比如分类、回归、序列预测），有些简单的指导原则来选择正确的损失函数。例如，二分类问题采用二值交叉熵损失，多分类问题采用分类交叉熵，回归问题选取均方误差，序列化学习问题选择联结时序分类（connectionist temporal classification，CTC）损失，等等。