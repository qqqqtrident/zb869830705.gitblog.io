---
layout: post
author: Robin
title: Python机器学习Ch3-05 --- 最大间隔分类之支持向量机
tags: 机器学习, Python
categories:
  - 机器学习 
  - Python
---

另一种比较流行和常用的机器学习算法是**支持向量机(SVM)**，SVM可以看做是感知机的扩展。在感知机算法中，优化目标是最小化错误分类误差，而在SVM中，优化目标是最大化间隔。间隔的定义是两个分割超平面(决策界)的距离，那些最接近超平面的训练样本也被称为**支持向量(supper vectors)**，如下图：

![](/assets/svm_graph.jpg)

# 最大化间隔

为什么要最大化决策界的间隔？这样做的原因是间隔大的决策界更容易过拟合。为了更好的理解间隔最大化，我们先认识一下那些和决策界平行的正超平面和负超平面，他们分别可表示为：

$$w_0 + w^Tx_{pos} = 1 \qquad (1)$$

$$w_0 + w^Tx_{neg} = -1 \qquad (2)$$

用公式*（1）*减去公式*（2）*，得到：

$$\Rightarrow w^T(x_{pos} - x_{neg}) = 2$$

我们可以通过向量*w*来标准化上述公式，如下：

$$\|w\| = \sqrt {\sum_{j=1}^m w_j^2}$$

进一步归一化后，得到：

$$\frac {w^T(x_{pos} - x_{neg})}{\|w\|} = \frac {2}{\|w\|}$$

上面公式中等号左边可以解释为正超平面和负超平面之间的距离，也就是所谓的间隔。

现在，SVM的目标函数就变成了最大化间隔\\(\frac {2}{\|w\|}\\)了，限制条件是样本被正确分类，可以写成：

$$w_0 + w^Tx^{(i)} \ge 1 \quad if \ y^{(i)} = 1$$

$$w_0 + w^Tx^{(i)} \lt -1 \quad if \ y^{(i)} = -1$$

上面两个限制条件表示所有负样本要落在负超平面那一侧，所有正样本要落在正超平面那一侧。我们用更简洁的写法可表示为：

$$y^{(i)}(w_0 + w^Tx^{(i)}) \ge 1 \ \forall
_i$$

>> 这里讲解SVM非常的简单，并没有深入，因此仅仅是一个了解。

> 文章内容来自《Python Machine Learning》
> 
> 由于正在学习，因此在记录过程中难免有误，请不吝指正批评，谢谢！