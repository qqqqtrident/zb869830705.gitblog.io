---
layout: post
author: Robin
title: Python机器学习Ch3-07 --- 使用核化SVM解决非线性问题
tags: 机器学习, Python
categories:
  - 机器学习 
  - Python
---

SVM之所以非常的受欢迎，另一个重要的原因是它很容易进行核化(kernelized)，能够解决非线性分类问题。在讨论核化SVM细节之前，我们首先来自己构造一个非线性的数据集，看看非线性分类问题是什么样子的。

使用如下的代码，我们可以构造一个简单点额数据集，其中100个样本为正类，100个样本为负类：

![](/assets/non_linear_q.png)

如上图所示，如果使用线性超平面来区分例子中的正类和负类是不可能的，所有前面在介绍线性逻辑回归和线性SVM都无法解决此类分类问题。

核化方法的思想是为了解决线性不可分数据，在原来的特征基础上创造出非线性的组合，然后利用映射函数\\(\phi(·)\\)将现有特征维度映射到更高维的特征空间，并且高维特征空间能够使得原来线性不可分数据编程线性可分的。

举个例子，在下图中，我们将两维的映射到三维特征空间，数据集也由线性不可分变成了线性可分，使用的映射函数为：

$$\phi(x_1,x_2) = (z_1,z_2,z_3) = (x_1, x_2, x_1^2 + x_2^2)$$

![](/assets/2d_2_x_d.jpg)

注意上图中，右上角图和右下角图的转变，高维空间中线性决策界实际上是低纬空间的非线性决策界，这个非线性决策界是线性分类器无法找得到的，而核方法能够找到。

# 使用核技巧在高维空间中找到可分超平面

使用SVM解决非线性问题，上面介绍了，会使用映射函数将训练集映射到高维特征空间，然后训练一个线性SVM模型在新的特征空间将数据分类。然后，然后可有使用相同的映射函数对测试集数据分类。

上面的想法不错，但是如何构造新特征是非常困难的，尤其是数据本身就是高维数据时。因此，我们就需要使用一些特殊的技巧---核技巧。由于我们不会过多的设计在训练SVM时如何求解二次规划问题，只需要知道用\\(\phi(x^{(i)})^T\phi(x^{(i)})\\)替换\\(x^{(i)T}x^{(j)}\\)就可以了。为了免去两个点的点乘计算，我们定义所谓的核函数（kernel function）:

$$k(x^{(i)}, x^{(j)}) = \phi(x^{(i)})^T\phi(x^{(j)})$$

常用的核函数是Radial Basis Function kernel(RBF核)，也称为高斯核：

$$k(x^{(i)}, x^{(j)}) = exp \biggl(-\frac{\|x^{(i)} - x^{(j)}\|^2}{2\sigma^2}\biggr)$$

通常简写为：

$$k(x^{(i)}, x^{(j)}) = exp \biggl(-\gamma	 \|x^{(i)} - x^{(j)}\|^2\biggr)$$

此处，\\(\gamma = \frac{1}{2\sigma^2}\\)是要优化的自由参数。

通俗的讲，核可以理解为两个样本之间的相似性函数。高斯核中的*e*的指数范围是\\(\le 0\\)，因此高斯核的值域范围\\(\in [0, 1]\\)，特别的，当两个样本完全一样时，值为1，完全不同时，值为0。

有了核函数的概念，我们就可以动手训练一个核SVM，看看能够对线性不可分的数据集是否能够正确分类：

![](/assets/kernel_svm.png)

如上图结果所示，核SVM在XOR数据集上表现不错。

其中参数gamma可以理解为高斯球面的阶段参数，如果我们增大gamma的值，会产生更加柔和的决策界，但是过大的gamma值带来的后果却是泛化能力的下降，因此选择适当的gamma值有助于避免过拟合的情况。


> 文章内容来自《Python Machine Learning》
> 
> 由于正在学习，因此在记录过程中难免有误，请不吝指正批评，谢谢！