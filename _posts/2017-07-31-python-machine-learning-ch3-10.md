---
layout: post
author: Robin
title: Python机器学习Ch3-10 --- 随机森林简单介绍
tags: 机器学习, Python
categories:
  - 机器学习 
  - Python
---

**随机森林(randomforest)**一直都是广受欢迎的机器学习算法，优点很多，比如优秀的分类表现、易扩展性和使用简单。随机森林的思想也不复杂，一个随机森林模型就是多颗决策树的集成。集成学习(ensemble learning)的观点是将多个弱分类器结合来构建一个强分类器，它的泛化误差小且不易过拟合。

随机森林算法大致分为4个步骤：

1. 通过自助法(bootstrap)构建大小为 *n* 的一个训练集，即重复抽样选择 *n* 个训练实例；
2. 对于得到的新数据集，构建一颗决策树。对于每个节点执行以下操作：
	1. 通过不重复抽样法，选择 *d* 个特征
	2. 利用选择得到的 *d* 个特征，选择某种度量分割节点
3. 重复以上步骤1和2，*k* 次；
4. 对于每一个测试样例，对 *k* 颗决策树预测结果进行投票。票数越多的结果就是随机森林的预测结果。

随机森林中构建决策树的做法和原始构建决策树的区别在于，在每次分割节点的时候，不是从所有特征中选择而是在一个小的特征集中选择特征。

虽然随机森林模型的可解释性不如决策树，但是它有一个很大的优点：受超参数的影响波动不是很大。也不需要对随机森林进行剪枝操作，因为集成模型的鲁棒性很强，不会过多受决策树噪声影响。

在实际使用随机森林模型时，树的数目 *k* 需要好好的调参。一般，树的数目越多，随机森林的性能越好，但是计算成本会增大。

样本大小 *n* 能控制bias-variance平衡，如果 *n* 很大，我们就减小随机性，否则会造成过拟合。另一方面，如果 *n* 很小，虽然不会过拟合，但是模型的性能会降低，大多数随机森林的实现中，*n* 的大小等于原始训练集的大小。

在每一次分割特征集的大小 *d*，都会小于原始特征集的大小，在sklearn中默认值是\\(d = m^{0.5}\\) ，其中 *m* 是原始特征集的大小，是一个比较合理的数值。

![](/assets/sample_random_forest.png)


>> 非常简单的一个介绍，详细使用可参考sklearn官网。


> 文章内容来自《Python Machine Learning》
> 
> 由于正在学习，因此在记录过程中难免有误，请不吝指正批评，谢谢！