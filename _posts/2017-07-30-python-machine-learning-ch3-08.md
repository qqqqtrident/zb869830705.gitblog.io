---
layout: post
author: Robin
title: Python机器学习Ch3-08 --- 决策树学习
tags: 机器学习, Python
categories:
  - 机器学习 
  - Python
---


如果我们在意模型的可解释性，那么**决策树(decision tree)**分类器绝对是最佳的选择。如果名字的字面意思一样，我们可以把决策树理解为基于一系列问题对数据做出的分割选择。

举一个简单的例子，我们使用决策树来决定某一天的活动：

![](/assets/activity_day.jpg)

基于训练集中的特征，决策树模型提出了一系列问题来推测样本的类别。虽然上图中做出的每个决策都是根据离散的变量，但也可以用于连续变量，比如，对于Iris中sepal width这一取值为实数的特征，我们可以为“sepal width 是否大于2.8cm？”等。

训练决策树模型时，我们从根节点开始，使用**信息增益(information gain, IG)**最大化的特征对数据进行分割，然后迭代此过程。显然，决策树的生成是一个递归过程，决策树基本算法中，有三种情况会导致递归返回：

1. 当前节点包含的样本全属于同一个类别，无需划分；
2. 当前属性集为空，或者所有样本在所有属性上取值相同，无法划分；
3. 当前节点包含的样本集合为空，不能划分。

每一个节点的样本都属于同一类，同时这也可能导致树的深度很大，节点很多，很容易引起过拟合。因此，剪枝操作时必不可少的，来控制树的深度。

# 最大信息增益

为了使用最大信息增益的特征分割数据，我们需要定义一个在决策树学习过程中的目标函数。此处，我们的目标函数是在每一次分割时的最大信息增益，定义如下：

$$IG(D_p,f) = I(D_p) - \sum_{j=1}^m \frac{N_j}{N_P}I(D_j)$$

其中，*f*是具体的特征，\\(D_pD_j\\)是当前数据集和用特征*f*分割后第*j*个子节点的数据集，*I*是某种度量，\\(N_p\\)是当前数据集的样本个数，\\(N_j\\)是第*j*个子节点的数据中样本的个数。为了简化和减小搜索空间，大多数决策树都是使用二叉树实现。这意味着每一个父节点都被分割为两个子节点，\\(D_{left}D_{right}\\)：

$$IG(D_p,a) = I(D_p) - \frac{N_{left}}{N_p}I(D_{left}) - \frac{N_{right}}{N_p}I(D_{right})$$

常用的度量 *i* 包括基尼系数(Gini index, \\(I_G\\))、熵(Entropy, \\(I_H\\))和分类错误(classification error, \\(I_E\\))。这里以熵为例：

$$I_H(t) = -\sum_{i=1}^c p(i|t)\log_2{p(i|t)}$$

这里的\\(p(i\|t)\\)指的是在节点 *t* 中属于类别 *c* 的样本占比。所有如果某个子节点中所有样本都属于同一类，则熵为 *0*，如果样本的类别是均匀分布，则熵最大。比如，在二分类情况下，如果\\(p(i=1\|t)=1\\)或者\\(p(i=0\|t)=0\\)，则熵为 *0*。因此，我们说熵的评价标准目的是最大化树中的关联信息。

基尼系数可以理解为最小化误差分类的概率：

$$I_G(t) = \sum_{i=1}^c p(i|t)(-p(i|t)) = 1 - \sum_{i=1}^c p(i|t)^2$$

和熵相同，如果节点中样本的类别均匀，则基尼系数最大，比如，在二分类的情况下：

$$1 = \sum_{i=1}^c 0.5^2 = 0.5$$

通常，熵和基尼系数的结果相似，所有不需要花太多时间在选择度量上面。

另一种常用的度量是分类误差：

$$I_E = 1 - \max \{p(i|t)\}$$

这个度量更建议在剪枝的时候使用，而不是在构建决策树的时候使用。举个例子，说明一下为什么不建议在构建决策树时使用：

![](/assets/3547_03_22.jpg)

如上图所示，\\(D_p\\)中包含40个正样本和40个样本，然后分割为两个子节点。信息增益使用分类误差作为度量，得到的值在A、B情况下相同，都是0.25，计算过程如下：

$$I_E(D_p) = 1 - 0.5 = 0.5$$

$$A:I_E(D_{left}) = 1 - \frac{3}{4} = 0.25$$

$$A:I_E(D_{right}) = 1 - \frac{3}{4} = 0.25$$

$$A:IG_E = 0.5 - \frac{4}{8}0.25 - \frac{4}{8}0.25 = 0.25$$

$$B:I_E(D_{left}) = 1 - \frac{4}{6} = \frac{1}{3}$$

$$B:I_E(D_{right}) = 1 - 1 = 0$$

$$B:IG_E = 0.5 - \frac{6}{8}\times\frac{1}{3} - 0 = 0.25$$

如果使用阻尼系数，则会按照B情况分割：

$$I_G(D_p) = 1 - (0.5^2 + 0.5^2) = 0.5$$

$$A:I_G(D_{left}) = 1 - ((\frac{3}{4})^2 + (\frac{1}{4})^2) = \frac{3}{8} = 0.375$$

$$A:I_G(D_{right}) = 1 - ((\frac{1}{4})^2 + (\frac{3}{4})^2) = \frac{3}{8} = 0.375$$

$$A:I_G = 0.5 - \frac{4}{8}0.375 - \frac{4}{8}0.375 = 0.125$$

$$B:I_G(D_{left}) = 1 - ((\frac{2}{6})^2 + (\frac{4}{6})^2) = \frac{4}{9} = \overline {0.4}$$

$$B:I_G(D_{righ}) = 1 - (1^2 + 0^2) = 0$$

$$B:IG_G = 0.5 - \frac{6}{8}\overline {0.4} - 0 = \overline {0.16}$$

同样的，如果使用熵作为度量，也会按照B进行分割：

$$I_H(D_p) = -(0.5\log_2(0.5) + 0.5\log_2(0.5)) = 1$$

$$A:I_H(D_{left}) = -(\frac{3}{4}\log_2(\frac{3}{4}) + \frac{1}{4}\log_2(\frac{1}{4})) = 0.81$$

$$A:I_H(D_{right}) = -(\frac{1}{4}\log_2(\frac{1}{4}) + \frac{3}{4}\log_2(\frac{3}{4})) = 0.81$$

$$A:IG_H = 1 - \frac{4}{8}0.81 - \frac{4}{8}0.81 = 0.19$$

$$B:I_H{D_{left}} = -(\frac{2}{6}\log_2(\frac{2}{6})  + \frac{4}{6}\log_2(\frac{4}{6})) = 0.92$$

$$B:I_H(D_{right}) = 0$$

$$B:IG_H = 1 - \frac{6}{8}0.92 - 0 = 0.31$$

接下来，我们用图示的方式，展现阻尼数、熵以及分类错误：

![](/assets/compare_3_var.png)



> 文章内容来自《Python Machine Learning》
> 
> 由于正在学习，因此在记录过程中难免有误，请不吝指正批评，谢谢！